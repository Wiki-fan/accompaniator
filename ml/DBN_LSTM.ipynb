{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DBN-LSTM.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "BRmGfezJajAe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bluG1sbyGO7o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline   \n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nfeIuXqyGO72",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Restricted Boltzmann Machine\n",
        "\"\"\"\n",
        "\n",
        "# Authors: Yann N. Dauphin <dauphiya@iro.umontreal.ca>\n",
        "#          Vlad Niculae\n",
        "#          Gabriel Synnaeve\n",
        "#          Lars Buitinck\n",
        "# License: BSD 3 clause\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from scipy.special import expit  # logistic function\n",
        "\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.externals.six.moves import xrange\n",
        "from sklearn.utils import check_array\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.utils import gen_even_slices\n",
        "from sklearn.utils import issparse\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "from sklearn.utils.extmath import log_logistic\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "\n",
        "class BernoulliRBM(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Bernoulli Restricted Boltzmann Machine (RBM).\n",
        "\n",
        "    A Restricted Boltzmann Machine with binary visible units and\n",
        "    binary hidden units. Parameters are estimated using Stochastic Maximum\n",
        "    Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n",
        "    [2].\n",
        "\n",
        "    The time complexity of this implementation is ``O(d ** 2)`` assuming\n",
        "    d ~ n_features ~ n_components.\n",
        "\n",
        "    Read more in the :ref:`User Guide <rbm>`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_components : int, optional\n",
        "        Number of binary hidden units.\n",
        "\n",
        "    learning_rate : float, optional\n",
        "        The learning rate for weight updates. It is *highly* recommended\n",
        "        to tune this hyper-parameter. Reasonable values are in the\n",
        "        10**[0., -3.] range.\n",
        "\n",
        "    batch_size : int, optional\n",
        "        Number of examples per minibatch.\n",
        "\n",
        "    n_iter : int, optional\n",
        "        Number of iterations/sweeps over the training dataset to perform\n",
        "        during training.\n",
        "\n",
        "    verbose : int, optional\n",
        "        The verbosity level. The default, zero, means silent mode.\n",
        "\n",
        "    random_state : integer or RandomState, optional\n",
        "        A random number generator instance to define the state of the\n",
        "        random permutations generator. If an integer is given, it fixes the\n",
        "        seed. Defaults to the global numpy random number generator.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    intercept_hidden_ : array-like, shape (n_components,)\n",
        "        Biases of the hidden units.\n",
        "\n",
        "    intercept_visible_ : array-like, shape (n_features,)\n",
        "        Biases of the visible units.\n",
        "\n",
        "    components_ : array-like, shape (n_components, n_features)\n",
        "        Weight matrix, where n_features in the number of\n",
        "        visible units and n_components is the number of hidden units.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "\n",
        "    >>> import numpy as np\n",
        "    >>> from sklearn.neural_network import BernoulliRBM\n",
        "    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "    >>> model = BernoulliRBM(n_components=2)\n",
        "    >>> model.fit(X)\n",
        "    BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,\n",
        "           random_state=None, verbose=0)\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "\n",
        "    [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for\n",
        "        deep belief nets. Neural Computation 18, pp 1527-1554.\n",
        "        http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\n",
        "\n",
        "    [2] Tieleman, T. Training Restricted Boltzmann Machines using\n",
        "        Approximations to the Likelihood Gradient. International Conference\n",
        "        on Machine Learning (ICML) 2008\n",
        "    \"\"\"\n",
        "    def __init__(self, n_components=256, learning_rate=0.1, batch_size=10,\n",
        "                 n_iter=10, verbose=0, random_state=None):\n",
        "        self.n_components = n_components\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.n_iter = n_iter\n",
        "        self.verbose = verbose\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Compute the hidden layer activation probabilities, P(h=1|v=X).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
        "            The data to be transformed.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        h : array, shape (n_samples, n_components)\n",
        "            Latent representations of the data.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"components_\")\n",
        "\n",
        "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
        "        return self._mean_hiddens(X)\n",
        "\n",
        "    def _mean_hiddens(self, v):\n",
        "        \"\"\"Computes the probabilities P(h=1|v).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        v : array-like, shape (n_samples, n_features)\n",
        "            Values of the visible layer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        h : array-like, shape (n_samples, n_components)\n",
        "            Corresponding mean field values for the hidden layer.\n",
        "        \"\"\"\n",
        "        p = safe_sparse_dot(v, self.components_.T)\n",
        "        p += self.intercept_hidden_\n",
        "        return expit(p, out=p)\n",
        "\n",
        "    def _sample_hiddens(self, v, rng):\n",
        "        \"\"\"Sample from the distribution P(h|v).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        v : array-like, shape (n_samples, n_features)\n",
        "            Values of the visible layer to sample from.\n",
        "\n",
        "        rng : RandomState\n",
        "            Random number generator to use.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        h : array-like, shape (n_samples, n_components)\n",
        "            Values of the hidden layer.\n",
        "        \"\"\"\n",
        "        p = self._mean_hiddens(v)\n",
        "        return (rng.random_sample(size=p.shape) < p)\n",
        "\n",
        "    def _sample_visibles(self, h, rng):\n",
        "        \"\"\"Sample from the distribution P(v|h).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        h : array-like, shape (n_samples, n_components)\n",
        "            Values of the hidden layer to sample from.\n",
        "\n",
        "        rng : RandomState\n",
        "            Random number generator to use.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        v : array-like, shape (n_samples, n_features)\n",
        "            Values of the visible layer.\n",
        "        \"\"\"\n",
        "        p = np.dot(h, self.components_)\n",
        "        p += self.intercept_visible_\n",
        "        expit(p, out=p)\n",
        "        return (rng.random_sample(size=p.shape) < p)\n",
        "\n",
        "    def _free_energy(self, v):\n",
        "        \"\"\"Computes the free energy F(v) = - log sum_h exp(-E(v,h)).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        v : array-like, shape (n_samples, n_features)\n",
        "            Values of the visible layer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        free_energy : array-like, shape (n_samples,)\n",
        "            The value of the free energy.\n",
        "        \"\"\"\n",
        "        return (- safe_sparse_dot(v, self.intercept_visible_)\n",
        "                - np.logaddexp(0, safe_sparse_dot(v, self.components_.T)\n",
        "                               + self.intercept_hidden_).sum(axis=1))\n",
        "\n",
        "    def gibbs(self, v):\n",
        "        \"\"\"Perform one Gibbs sampling step.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        v : array-like, shape (n_samples, n_features)\n",
        "            Values of the visible layer to start from.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        v_new : array-like, shape (n_samples, n_features)\n",
        "            Values of the visible layer after one Gibbs step.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"components_\")\n",
        "        if not hasattr(self, \"random_state_\"):\n",
        "            self.random_state_ = check_random_state(self.random_state)\n",
        "        h_ = self._sample_hiddens(v, self.random_state_)\n",
        "        v_ = self._sample_visibles(h_, self.random_state_)\n",
        "\n",
        "        return v_\n",
        "\n",
        "    def partial_fit(self, X, y=None):\n",
        "        \"\"\"Fit the model to the data X which should contain a partial\n",
        "        segment of the data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Training data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : BernoulliRBM\n",
        "            The fitted model.\n",
        "        \"\"\"\n",
        "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
        "        if not hasattr(self, 'random_state_'):\n",
        "            self.random_state_ = check_random_state(self.random_state)\n",
        "        if not hasattr(self, 'components_'):\n",
        "            self.components_ = np.asarray(\n",
        "                self.random_state_.normal(\n",
        "                    0,\n",
        "                    0.01,\n",
        "                    (self.n_components, X.shape[1])\n",
        "                ),\n",
        "                order='F')\n",
        "        if not hasattr(self, 'intercept_hidden_'):\n",
        "            self.intercept_hidden_ = np.zeros(self.n_components, )\n",
        "        if not hasattr(self, 'intercept_visible_'):\n",
        "            self.intercept_visible_ = np.zeros(X.shape[1], )\n",
        "        if not hasattr(self, 'h_samples_'):\n",
        "            self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
        "\n",
        "        self._fit(X, self.random_state_)\n",
        "\n",
        "    def _fit(self, v_pos, rng):\n",
        "        \"\"\"Inner fit for one mini-batch.\n",
        "\n",
        "        Adjust the parameters to maximize the likelihood of v using\n",
        "        Stochastic Maximum Likelihood (SML).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        v_pos : array-like, shape (n_samples, n_features)\n",
        "            The data to use for training.\n",
        "\n",
        "        rng : RandomState\n",
        "            Random number generator to use for sampling.\n",
        "        \"\"\"\n",
        "        h_pos = self._mean_hiddens(v_pos)\n",
        "        v_neg = self._sample_visibles(self.h_samples_, rng)\n",
        "        h_neg = self._mean_hiddens(v_neg)\n",
        "\n",
        "        lr = float(self.learning_rate) / v_pos.shape[0]\n",
        "        update = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T\n",
        "        update -= np.dot(h_neg.T, v_neg)\n",
        "        self.components_ += lr * update\n",
        "        self.intercept_hidden_ += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n",
        "        self.intercept_visible_ += lr * (np.asarray(\n",
        "                                         v_pos.sum(axis=0)).squeeze() -\n",
        "                                         v_neg.sum(axis=0))\n",
        "\n",
        "        h_neg[rng.uniform(size=h_neg.shape) < h_neg] = 1.0  # sample binomial\n",
        "        self.h_samples_ = np.floor(h_neg, h_neg)\n",
        "\n",
        "    def score_samples(self, X):\n",
        "        \"\"\"Compute the pseudo-likelihood of X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
        "            Values of the visible layer. Must be all-boolean (not checked).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pseudo_likelihood : array-like, shape (n_samples,)\n",
        "            Value of the pseudo-likelihood (proxy for likelihood).\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        This method is not deterministic: it computes a quantity called the\n",
        "        free energy on X, then on a randomly corrupted version of X, and\n",
        "        returns the log of the logistic function of the difference.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"components_\")\n",
        "\n",
        "        v = check_array(X, accept_sparse='csr')\n",
        "        rng = check_random_state(self.random_state)\n",
        "\n",
        "        # Randomly corrupt one feature in each sample in v.\n",
        "        ind = (np.arange(v.shape[0]),\n",
        "               rng.randint(0, v.shape[1], v.shape[0]))\n",
        "        if issparse(v):\n",
        "            data = -2 * v[ind] + 1\n",
        "            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)\n",
        "        else:\n",
        "            v_ = v.copy()\n",
        "            v_[ind] = 1 - v_[ind]\n",
        "\n",
        "        fe = self._free_energy(v)\n",
        "        fe_ = self._free_energy(v_)\n",
        "        return v.shape[1] * log_logistic(fe_ - fe)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the model to the data X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
        "            Training data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : BernoulliRBM\n",
        "            The fitted model.\n",
        "        \"\"\"\n",
        "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
        "        n_samples = X.shape[0]\n",
        "        rng = check_random_state(self.random_state)\n",
        "\n",
        "        self.components_ = np.asarray(\n",
        "            rng.normal(0, 0.01, (self.n_components, X.shape[1])),\n",
        "            order='F')\n",
        "        #self.intercept_hidden_ = np.zeros(self.n_components, )\n",
        "        #self.intercept_visible_ = np.zeros(X.shape[1], )\n",
        "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
        "\n",
        "        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n",
        "        batch_slices = list(gen_even_slices(n_batches * self.batch_size,\n",
        "                                            n_batches, n_samples))\n",
        "        verbose = self.verbose\n",
        "        begin = time.time()\n",
        "        for iteration in xrange(1, self.n_iter + 1):\n",
        "            for batch_slice in batch_slices:\n",
        "                self._fit(X[batch_slice], rng)\n",
        "\n",
        "            if verbose:\n",
        "                end = time.time()\n",
        "                print(\"[%s] Iteration %d, pseudo-likelihood = %.2f,\"\n",
        "                      \" time = %.2fs\"\n",
        "                      % (type(self).__name__, iteration,\n",
        "                         self.score_samples(X).mean(), end - begin))\n",
        "                begin = end\n",
        "\n",
        "        return self\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gNBISF1FGO7-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0fa9438f-9977-4d8a-e0b9-c8b994a6df84",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526376041190,
          "user_tz": -180,
          "elapsed": 928,
          "user": {
            "displayName": "Valeriia Nemychnikova",
            "photoUrl": "//lh4.googleusercontent.com/-U60a1KIeug0/AAAAAAAAAAI/AAAAAAAAJlk/s7QVyejxEI0/s50-c-k-no/photo.jpg",
            "userId": "101878140320600604863"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"hello\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_PIFUSXQGhrw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "class DBN(BaseEstimator):\n",
        "    def __init__(self, n_components_rbm=256, n_layers=3):\n",
        "        self.rbms = []\n",
        "        for i in range(n_layers):\n",
        "            self.rbms.append(BernoulliRBM(n_components=n_components_rbm))\n",
        "        self.n_comp = n_components_rbm\n",
        "        self.n_layers = n_layers\n",
        "    def fit(self, X, biases_hidden, biases_visible):\n",
        "        for i in range(self.n_layers):\n",
        "            self.rbms[i].intercept_hidden_ = biases_hidden\n",
        "            self.rbms[i].intercept_visible_ = biases_visible\n",
        "        y = X\n",
        "        for rbm in self.rbms:\n",
        "            print(y.shape)\n",
        "            y = rbm.fit_transform(y)\n",
        "    def transform(self, X):\n",
        "        y = X\n",
        "        for rbm in self.rbms:\n",
        "            y = rbm.transform(y)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DYKj43yaGO8I",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NXJKUPLbGO8O",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from scipy.stats import bernoulli\n",
        "X = bernoulli.rvs(0.01, size=(100, 20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sKrJqTskV_BU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y = bernoulli.rvs(0.01, size=(1, 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ezqIxN6Kp3G",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5e896ca6-c2d8-4508-dd73-f34d9854f142",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526377557140,
          "user_tz": -180,
          "elapsed": 584,
          "user": {
            "displayName": "Valeriia Nemychnikova",
            "photoUrl": "//lh4.googleusercontent.com/-U60a1KIeug0/AAAAAAAAAAI/AAAAAAAAJlk/s7QVyejxEI0/s50-c-k-no/photo.jpg",
            "userId": "101878140320600604863"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "dbn = DBN(n_components_rbm=20)\n",
        "dbn.fit(X, np.ones(20), np.zeros(20))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 20)\n",
            "(100, 20)\n",
            "(100, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c2JrAQyDLsFW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "67be0151-ef47-48cd-eb0d-7a390c3911d1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526377559866,
          "user_tz": -180,
          "elapsed": 652,
          "user": {
            "displayName": "Valeriia Nemychnikova",
            "photoUrl": "//lh4.googleusercontent.com/-U60a1KIeug0/AAAAAAAAAAI/AAAAAAAAJlk/s7QVyejxEI0/s50-c-k-no/photo.jpg",
            "userId": "101878140320600604863"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "dbn.transform(X)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.97834013, 0.97771354, 0.97684312, ..., 0.97472024, 0.9779636 ,\n",
              "        0.9771036 ],\n",
              "       [0.97834013, 0.97771354, 0.97684312, ..., 0.97472024, 0.9779636 ,\n",
              "        0.9771036 ],\n",
              "       [0.97834013, 0.97771354, 0.97684312, ..., 0.97472024, 0.9779636 ,\n",
              "        0.9771036 ],\n",
              "       ...,\n",
              "       [0.97834013, 0.97771354, 0.97684312, ..., 0.97472024, 0.9779636 ,\n",
              "        0.9771036 ],\n",
              "       [0.97801544, 0.97738524, 0.97650687, ..., 0.97436663, 0.97763634,\n",
              "        0.97676991],\n",
              "       [0.97834013, 0.97771354, 0.97684312, ..., 0.97472024, 0.9779636 ,\n",
              "        0.9771036 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "tnd7tXtlYSvQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "e6281e03-e259-411e-e233-1e6ddc54251f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526380726452,
          "user_tz": -180,
          "elapsed": 740,
          "user": {
            "displayName": "Valeriia Nemychnikova",
            "photoUrl": "//lh4.googleusercontent.com/-U60a1KIeug0/AAAAAAAAAAI/AAAAAAAAJlk/s7QVyejxEI0/s50-c-k-no/photo.jpg",
            "userId": "101878140320600604863"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cross_validation import train_test_split\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.engine.topology import Input\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.layers import Concatenate, Merge\n",
        "\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "On6T_RJfM3-u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "b8946354-52f6-46b7-b33f-4da048ac31ca",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526380733728,
          "user_tz": -180,
          "elapsed": 854,
          "user": {
            "displayName": "Valeriia Nemychnikova",
            "photoUrl": "//lh4.googleusercontent.com/-U60a1KIeug0/AAAAAAAAAAI/AAAAAAAAJlk/s7QVyejxEI0/s50-c-k-no/photo.jpg",
            "userId": "101878140320600604863"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model_chords = Sequential()\n",
        "model_notes = Sequential()\n",
        "\n",
        "model_chords.add(Embedding(10000, 20, input_length=1, batch_input_shape=(1,1)))\n",
        "model_notes.add(Embedding(128, 20, input_length=1, batch_input_shape=(1,1)))\n",
        "merged_model = Sequential()\n",
        "merged_model.add(Merge([model_chords, model_notes], mode=\"concat\"))\n",
        "merged_model.add(LSTM(20, stateful=True))\n",
        "merged_model.add(Dense(10000, activation='softmax'))\n",
        "merged_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "x1DjiuSnPuZA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "60c48375-26ff-4218-de5a-7ecb0d88611c",
        "executionInfo": {
          "status": "error",
          "timestamp": 1526380669250,
          "user_tz": -180,
          "elapsed": 602,
          "user": {
            "displayName": "Valeriia Nemychnikova",
            "photoUrl": "//lh4.googleusercontent.com/-U60a1KIeug0/AAAAAAAAAAI/AAAAAAAAJlk/s7QVyejxEI0/s50-c-k-no/photo.jpg",
            "userId": "101878140320600604863"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, Add, Sequential\n",
        "from keras.models import Model\n",
        "\n",
        "main_input = Input(shape=(100,), dtype='int32', name='main_input',  batch_shape=(1,20))\n",
        "main_input_notes = Embedding(128, 20, input_length=1, batch_input_shape=(1,1))(main_input)\n",
        "auxiliary_input = Input(shape=(20,), name='aux_input')\n",
        "aux_in = Dense(20)(auxiliary_input)\n",
        "x = Add()([main_input_notes, aux_in])\n",
        "\n",
        "lstm_out = LSTM(20, stateful=True, return_sequences=True)(x)\n",
        "auxiliary_output = Dense(20, activation='relu', name='aux_output')(lstm_out)\n",
        "\n",
        "converted = Dense(10000, activation='tanh')(lstm_out)\n",
        "main_output = Dense(1, activation='sigmoid', name='main_output')(converted)\n",
        "\n",
        "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
        "              loss_weights=[1., 0.1])\n",
        "\n",
        "\n",
        "#model.fit([headline_data, additional_data], [labels, labels],\n",
        " #         epochs=50, batch_size=32)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-f0b12c89818a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Note that we can name any layer by passing it a \"name\" argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Sequential'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Fmpb_sMyVuJk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "ac61c8fe-e110-403f-e4ca-cf347a8599e7",
        "executionInfo": {
          "status": "error",
          "timestamp": 1526380409422,
          "user_tz": -180,
          "elapsed": 642,
          "user": {
            "displayName": "Valeriia Nemychnikova",
            "photoUrl": "//lh4.googleusercontent.com/-U60a1KIeug0/AAAAAAAAAAI/AAAAAAAAJlk/s7QVyejxEI0/s50-c-k-no/photo.jpg",
            "userId": "101878140320600604863"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit([X, dbn.transform(X)], [y, y], epochs=1, batch_size=1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-4f77561c6cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected main_output to have 3 dimensions, but got array with shape (1, 100)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dINNFXJ2MY5e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class DBNLSTM(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit(self, X, y):\n",
        "        pass #here we need to join DBN and LSTM\n",
        "    #merged_model.fit([X, X_notes], y, epochs=1, batch_size=1, verbose=1)\n",
        "     #           merged_model.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h7yOcYo-GO8U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder()\n",
        "X = enc.fit_transform(X + 1)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YkmMBPOkGO8a",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SqQTVUovGO8k",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "outputId": "0fc4797e-03dd-4224-d61b-84640a24d147"
      },
      "cell_type": "code",
      "source": [
        "rbm = BernoulliRBM(n_components=100, learning_rate=0.01, random_state=0, verbose=True)\n",
        "rbm.intercept_hidden_ = np.zeros(rbm.n_components, )\n",
        "rbm.intercept_visible_ = np.zeros(X.shape[1], )\n",
        "rbm.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -98.44, time = 1.32s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -99.15, time = 1.46s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -91.74, time = 1.54s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -84.56, time = 1.50s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -79.34, time = 1.51s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -74.15, time = 1.51s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -69.49, time = 1.46s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -64.49, time = 1.45s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -60.76, time = 1.48s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -57.00, time = 1.46s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BernoulliRBM(batch_size=10, learning_rate=0.01, n_components=100, n_iter=10,\n",
              "       random_state=0, verbose=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}